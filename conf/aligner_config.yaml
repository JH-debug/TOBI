# This config contains the default values for training Aligner model on LJSpeech dataset.
# If you want to train model on other dataset, you can change config values according to your dataset.
# Most dataset-specific arguments are in the head of the config file, see below.

name: Aligner

train_dataset: '../processed/tts_align_train.json'
validation_datasets: '../processed/tts_align_val.json'
sup_data_path: '../processed/'
sup_data_types: [ "align_prior_matrix", "pitch" ]

# Default values for dataset with sample_rate=22050
sample_rate: 44100
n_mel_channels: 80
n_window_size: 2048
n_window_stride: 512
n_fft: 2048
lowfreq: 0
highfreq: null
window: hann

batch_size: 16
num_workers: 1

text_tokenizer:
  _target_: korean_tokenizer.SimpleTokenizer
  manifest_path: '../processed/manifest_tts.json'
  token_list_path: '../processed/token_list.txt'


model:
  symbols_embedding_dim: 384
  bin_loss_start_ratio: 0.2
  bin_loss_warmup_epochs: 100

  sample_rate: ${sample_rate}
  n_mel_channels: ${n_mel_channels}
  n_window_size: ${n_window_size}
  n_window_stride: ${n_window_stride}
  n_fft: ${n_fft}
  lowfreq: ${lowfreq}
  highfreq: ${highfreq}
  window: ${window}


  text_tokenizer:
    _target_: korean_tokenizer.SimpleTokenizer
    manifest_path: '../processed/manifest_tts.json'
    token_list_path: '../processed/token_list.txt'

  train_ds:
    dataset:
      _target_: nemo.collections.tts.torch.data.TTSDataset
      manifest_filepath: ${train_dataset}
      sample_rate: ${model.sample_rate}
      sup_data_path: ${sup_data_path}
      sup_data_types: ${sup_data_types}
      n_fft: ${model.n_fft}
      win_length: ${model.n_window_size}
      hop_length: ${model.n_window_stride}
      window: ${model.window}
      n_mels: ${model.n_mel_channels}
      lowfreq: ${model.lowfreq}
      highfreq: ${model.highfreq}
      max_duration: null
      min_duration: 0.1
      ignore_file: null
      trim: false
      # text_tokenizer: ${text_tokenizer}

    dataloader_params:
      drop_last: false
      shuffle: true
      batch_size: ${batch_size}
      num_workers: ${num_workers}
      pin_memory: true

  validation_ds:
    dataset:
      _target_: nemo.collections.tts.torch.data.TTSDataset
      manifest_filepath: ${validation_datasets}
      sample_rate: ${model.sample_rate}
      sup_data_path: ${sup_data_path}
      sup_data_types: ${sup_data_types}
      n_fft: ${model.n_fft}
      win_length: ${model.n_window_size}
      hop_length: ${model.n_window_stride}
      window: ${model.window}
      n_mels: ${model.n_mel_channels}
      lowfreq: ${model.lowfreq}
      highfreq: ${model.highfreq}
      max_duration: null
      min_duration: 0.1
      ignore_file: null
      trim: false

    dataloader_params:
      drop_last: false
      shuffle: false
      batch_size: ${batch_size}
      num_workers: ${num_workers}
      pin_memory: true

  preprocessor:
    _target_: nemo.collections.asr.modules.AudioToMelSpectrogramPreprocessor
    features: ${model.n_mel_channels}
    lowfreq: ${model.lowfreq}
    highfreq: ${model.highfreq}
    n_fft: ${model.n_fft}
    n_window_size: ${model.n_window_size}
    window_size: false
    n_window_stride: ${model.n_window_stride}
    window_stride: false
    pad_to: 1
    pad_value: -11.52
    sample_rate: ${model.sample_rate}
    window: ${model.window}
    normalize: null
    preemph: null
    dither: 0.0
    frame_splicing: 1
    log: true
    log_zero_guard_type: clamp
    log_zero_guard_value: 1e-05
    mag_power: 1.0

  alignment_module:
    _target_: nemo.collections.tts.modules.aligner.AlignmentEncoder
    n_mel_channels: ${model.n_mel_channels}
    n_text_channels: ${model.symbols_embedding_dim}
    n_att_channels: ${model.n_mel_channels}

  duration_predictor:
    _target_: nemo.collections.tts.modules.fastpitch.TemporalPredictor
    input_size: ${model.symbols_embedding_dim}
    kernel_size: 3
    filter_size: 256
    dropout: 0.1
    n_layers: 2

  optim:
    name: adam
    lr: 1e-3
    betas: [ 0.9, 0.999 ]
    weight_decay: 1e-6

    sched:
      name: NoamAnnealing
      warmup_steps: 1000
      last_epoch: -1
      d_model: 1

trainer:
  num_nodes: 1
  devices: 1 # [1,2,3] # 1
  num_nodes: 1
  accelerator: gpu
  strategy: ddp
  precision: 32
  max_epochs: 1000
  accumulate_grad_batches: 1
  gradient_clip_val: 1000.0
  enable_checkpointing: false # Provided by exp_manager
  logger: false # Provided by exp_manager
  log_every_n_steps: 100
  check_val_every_n_epoch: 1
  benchmark: false

exp_manager:
  exp_dir: null
  name: ${name}
  create_tensorboard_logger: true
  create_checkpoint_callback: true
  checkpoint_callback_params:
    monitor: val_forward_sum_loss
    mode: min
  create_wandb_logger: false
  wandb_logger_kwargs:
    name: null
    project: null
    entity: null
  resume_if_exists: false
  resume_ignore_no_checkpoint: false